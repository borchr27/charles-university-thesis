\chapter*{Introduction}
\addcontentsline{toc}{chapter}{Introduction}

One of the main challenges of creating a successful machine learning model is obtaining labeled data. With easy access to a variety of modern tools, devices, and sensors, we are able to rapidly collect unlabeled data. But, in supervised learning, prediction models are trained using labeled data. The problem is that acquiring labels for the collected data can be expensive, time-consuming, or even impossible in some cases.  

However, methods have been developed to help reduce the number of labeled data required to train the classifier. Active learning is a semi-supervised machine learning framework where the model is trained with a smaller set of labeled data but which also aims to exploit trends within the unlabeled data. Active learning is a framework in which the learner has the freedom to select which data points are added to its training set (\cite{roy2001eer}). 

Active learning is different from other frameworks because it uses the unlabeled data and some evaluation criteria to determine which candidate could be the most beneficial to the model if it was given a label. In summary, the model requests the label from some oracle that provides the label then it takes this new labeled data point and rebuilds the classifier. We describe it as semi supervised active learning because of the oracle (typically a human) involved in the process that provides the label for the requested candidate data. 

In our case we will provide a set of labeled data to the active learning framework (or sampling strategy). The sampling strategy will assume all the data is unlabeled and then choose a candidate from unlabeled data pool. Then the label is revealed and the classifier is updated using the new data point.

In our case we have some data (website urls) for some company or business that are given to us from our partner. From this data our partner currently utilizes human labor to browse the website and then label the url with a category (~23 labels) and a sub-category (~234+ tags) that branch from the main category but still have some relation. This is a repetitive and expensive task that could be automated using active learning.

To reduce the burden of human labeling we propose using a combination of tools, namely, Scrapy, Postgres, translation services, and semi supervised active learning that require occasional interaction where a human can label a candidate (if unlabeled) that is expected to be most beneficial to the classifier. 

A website is required as input, then we use Scrapy to navigate to the webpage, collect and store the scraped data into the database. Next we access the data, translate the text, and add the translated data back into the database. We then create the model using the data from the database. 

In the first section we introduce active learning and the different components of active learning. In the second and third sections we discuss the details of xPAL and the process of collecting and preparing the data, respectively. In the fourth section we look at combinations of different sampling strategies / classifiers and their performance results.

\section*{Notable Definitions}

In this section we define some terms and ideas that will be helpful in understanding the upcoming sections.

\begin{defn}[Beta Prior]
\label{def:beta_prior}
A beta prior is a conjugate prior for the binomial distribution. It is a continuous probability distribution defined on the interval [0, 1] and is parameterized by two positive shape parameters, \(\alpha\) and \(\beta\). The beta distribution is defined as: 
\[\text{Beta}(\alpha, \beta) = \frac{\Gamma(\alpha + \beta)}{\Gamma(\alpha)\Gamma(\beta)}x^{\alpha - 1}(1 - x)^{\beta - 1}\]
where \(\Gamma\) is the gamma function and \(x\) is a random variable.
\end{defn}

\begin{defn}[Conjugate Prior]
\label{def:conjugate_prior}
A conjugate prior is a prior distribution that has the same functional form as the likelihood function. In other words, the posterior distribution will have the same functional form as the prior distribution.
\end{defn}

\begin{defn}[Decision-Theoretic]
\label{def:decision_theoretic}
Decision-theoretic active learning is a framework that uses the expected performance gain of a candidate to determine which candidate to label. The expected performance gain is the expected performance of the classifier after labeling the candidate minus the expected performance of the classifier before labeling the candidate. The expected performance of the classifier is the expected value of the performance measure given the posterior distribution of the classifier.
\end{defn}

\begin{defn}[Dirichlet Distribution]
\label{def:dirichlet_distribution}
The Dirichlet distribution is a multivariate generalization of the beta distribution. It is a continuous probability distribution defined on the \(K\)-simplex, \(\Delta_K = \{x \in \mathbb{R}^K: x_i \geq 0, \sum_{i=1}^K x_i = 1\}\). The Dirichlet distribution is parameterized by a vector of positive shape parameters, \(\alpha = (\alpha_1, \alpha_2, \dots, \alpha_K)\). The Dirichlet distribution is defined as:
\[\text{Dir}(\alpha) = \frac{\Gamma(\sum_{i=1}^K \alpha_i)}{\prod_{i=1}^K \Gamma(\alpha_i)}\prod_{i=1}^K x_i^{\alpha_i - 1}\]
where \(\Gamma\) is the gamma function and \(x\) is a random vector. The gamma function is defined as:
\[\Gamma(x) = \int_0^\infty t^{x - 1}e^{-t}dt\]
The gamma function is used as a normalizing constant to ensure that the probability density function integrates to 1 over the simplex, which is the space of all probability vectors that sum to 1.
\end{defn}

\begin{defn}[Ground Truth]
\label{def:ground_truth}
Ground truth is the true label of a data point.
\end{defn}

\begin{defn}[Posterior Probabilities]
\label{def:posterior_probabilities}
Posterior probability is a type of conditional probability that results from updating the prior probability with information summarized by the likelihood via an application of Bayes' rule. The posterior probability is the probability of an event occurring given that another event has occurred.
\end{defn}

\begin{defn}[Omniscient Oracles]
\label{def:omniscient_oracles}
Omniscient oracle is a hypothetical entity that has complete knowledge of the true labels of all data points in a given dataset. An omniscient oracle knows the ground truth labels of all data points.
\end{defn}

