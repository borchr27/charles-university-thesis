\chapter{Active Learning}



\section{Introduction}
Russel and Norvig succinctly define an agent and different types of learning in their book "Artificial Intelligence: A Modern Approach" (\cite{russell2009artificial}), their definition is paraphrased here. They define an agent as something that acts and a rational agent as one that acts so as to achieve the best outcome. If there is uncertainty, then the agent tries to achieve the best expected outcome. Any component of an agent can be improved by learning from data. The improvements and techniques used to make them depend on four major factors:

\begin{enumerate}
  \item Which component is to be improved.
  \item What prior knowledge the agent already has.
  \item What representation is used for the data and the component.
  \item What feedback is available to learn from.  
\end{enumerate}

Here we will mostly be focused on the final point, "What feedback is available to learn from". However, we will also discuss the importance of the second and third points because we will use Bayesian learning. There are three main types of feedback that determine the three main types of learning, which are: unsupervised, reinforcement, and supervised. 

In unsupervised learning an agents goal is to discover patterns in the data even though no feedback or labels are provided. In reinforcement learning, the agent learns from a series of rewards or punishments that are dealt out based on its decisions. In supervised learning, an agent learns from input-output pairs, which can be discrete or continuous, to find a function that maps the pairs as best as possible. 

The goal of supervised learning, given a training set of $N$ example input-output pairs:

\[(x_1, y_1), (x_2,y_2),... (x_N,y_N),\]

where each $y_j$ was generated by some unknown function $y=f(x)$, is to find a function $h$ that approximates the true function $f$.

In reality, the types of learning overlap. In semi-supervised learning, some data points are labeled, and some are not. The model is trained on the labeled data, and then the knowledge gained from that labeled data is used to improve the model's predictions on the unlabeled data. 

Supervised learning models almost always get more accurate with more labeled data. Active learning is the process of deciding which data to select for annotation (\cite{munro2021human}). In other words, the central component of an active learning algorithm is the selection strategy, or deciding which of the unlabeled data could be the most useful to the model if it was labeled. Active learning uses a selection strategy that augments the existing classifier, it is not itself a classifier but rather an evaluation methodology working with a classifier.

Many different sampling strategies exist. First we will discuss query functions then we will briefly define three basic sampling strategies: uncertainty, diversity, and random sampling to get an idea of sampling. We will then discuss some other more advanced sampling strategies that are used in our experiments. When sampling the unlabeled data an ordered list is returned and the top candidate is the candidate that is expected to be most valuable for the model, but we are not strictly limited to taking just one candidate.

\section{Query Function Construction}

There are various techniques used to construct the querying functions. We will focus on pool-based active learning, but a number of interesting and relevant ideas appear within other active-learning frameworks that are worth mentioning.

\subsection{Pool-Based}
In pool-based active learning, a fixed set of unlabeled examples is provided at the start of the learning process, and the active learner iteratively selects a subset of these examples for annotation (\cite{huang2016alce}). The selection of the subset is based on a query strategy that aims to maximize the information gain from each annotation. Pool-based active learning is useful in situations where all the data is available in advance, such as in document classification or image classification.

\subsection{Stream-Based}
In stream-based active learning, data arrives in a continuous stream, and the active learner must make real-time decisions about which examples to label (\cite{baram2004online}). This is common in settings such as sensor networks or social media feeds. The selection of examples for annotation is based on a query strategy that takes into account the current state of the model, as well as the uncertainty and informativeness of each incoming example. The stream-based model can be viewed as an online version of the pool-based model. 

\subsection{Membership Queries}

In membership-query-based active learning, the active learner can make queries to an oracle or construct a point in input space and requests its label from an oracle, such as a human expert, to obtain labels for specific examples (\cite{baram2004online}). The goal is to select the examples for which obtaining a label is most informative, in order to minimize the number of queries required to achieve a high accuracy. Membership-query-based active learning is useful when labeling each example is expensive or time-consuming, such as in medical diagnosis or legal document review.

\section{Sampling Strategies}

Sampling strategies, also referred to as selection strategies, are the core of the active learning process. The goal of sampling is to select the most useful data points from the unlabeled pool to label. The most useful data points are those that are expected to improve the classifier the most.

\subsection{Random Sampling}

Random sampling is self explanatory as it randomly selects an unlabeled data point from the pool and requests to have it labeled then it uses this newly selected data point to update the model. Random sampling is good to use as a baseline to compare other sampling strategies with.

\subsection{Diversity Sampling}

Diversity sampling is the set of strategies for identifying unlabeled items that are underrepresented or unknown to the machine learning model in its current state (\cite{munro2021human}). The items may have features that are unique or obscure in the training data, or they might represent data that are currently under-represented in the model. 

Either way this can result in poor or uneven performance when the model is applied or the data is changing over time. The goal of diversity sampling is to target new, unusual, or underrepresented items for annotation to give the algorithm a more complete picture of the problem space. 

\subsection{Uncertainty Sampling}

Uncertainty sampling is based on the idea that the most informative examples to query are the ones that the current model is most uncertain about. For example, in binary classification, an uncertain example might be one that is close to the decision boundary, or one that has a low predicted probability for the majority class (\cite{munro2021human}). The idea is that by querying these uncertain examples, the model can better learn the boundary between the classes and improve its accuracy. Uncertainty sampling is simple given a classifier that estimates $P (C|w)$ (\cite{lewis1994uncertainty}). On each iteration, the current version of classifier can be applied to each data point, and the data with estimated $P(C|w)$ values closest to 0.5 are selected, since 0.5 corresponds to the classifier being most uncertain of the class label.

These items are most likely to be wrongly classified, so they are the most likely to result in a label that differs from the predicted label, moving the decision boundary after they have been added to the training data and the model has been retrained.

\subsection{EER}
Monte Carlo estimation of error reduction (EER) estimates future error rate by log-loss, using the entropy of the posterior class distribution on a sample of the unlabeled examples, or by 0-1 loss, using the posterior probabilities of the most probable class for the sampled unlabeled examples (\cite{roy2001eer}).

Basically, the goal is to estimate the expected reduction in error for each unlabeled example by randomly sampling from the model's predictions and comparing the performance of the model with and without the example included in the training data.

\subsection{PAL}

Probabilistic Active Learning (PAL) follows a smoothness assumption and models for a candidate instance both the true posterior in its neighborhood and its label as random variables (\cite{kottke2014pal}). By computing for each candidate its expected gain in classification performance over both variables, PAL selects the candidate for labeling that is optimal in expectation. PAL shows comparable or better classification performance than error reduction and uncertainty sampling, has the same asymptotic linear time complexity as uncertainty sampling, and its faster than error reduction based on the tests from the paper.

\subsection{xPAL}

Extended probabilistic gain for active learning (xPAL) is a decision-theoretic selection strategy that directly optimizes the gain and misclassification error, and uses a Bayesian approach by introducing a conjugate prior distribution to determine the class posterior to deal with uncertainties (\cite{kottke2021toward}). Although the data distribution can be estimated, there is still uncertainty about the true class posterior probabilities. 

These class posterior probabilities can be modeled as a random variable based on the current observations in the dataset. For this model, a Bayesian approach is used by incorporating a conjugate prior to the observations. This produces more robust usefulness estimates for the candidates.

\subsection{ALCE}

Active Learning with Cost Embedding (ALCE) is a non-probabilistic uncertainty sampling algorithm for cost-sensitive multiclass active learning (\cite{huang2016alce}). First a cost-sensitive multiclass classification algorithm called cost embedding (CE) was designed, which embeds the cost information in the distance measure in a special hidden space by non-metric multidimensional scaling. Then a mirroring trick was used to let CE embed the possibly asymmetric cost information in the symmetric distance measure.

It works by augmenting the example space with an additional dimension that represents the cost of labeling each example. This cost embedding can be learned from previous labeling efforts or estimated based on domain knowledge. The cost embedding can then be used to guide the active learning process by selecting examples that are not only informative but also cost-effective to label.

\subsection{QBC}
Query By Committee (QBC) uses an ensemble of classifiers that are trained on bootstrapped replicates of the labeled set (\cite{seung1992qbc}). The idea is to train a committee of classifiers on the available labeled data and then use the committee to select the most informative unlabeled data for labeling (\cite{freund1997qbc}). The committee consists of several classifiers, each trained on a slightly different subset of the available labeled data.

The QBC algorithm measures the disagreement of the committee's predictions on each unlabeled data point. The intuition is that if the committee members disagree then it is likely to be a difficult data point for the current classifier and thus informative for labeling.

The algorithm selects a fixed number of the most informative examples and requests their labels. The labeled examples are then added to the labeled pool, and the committee is retrained on the expanded labeled pool. This process is repeated until the algorithm achieves a desired level of accuracy or the available labeling budget is exhausted.

\section{Classifiers}

The classifier integrated into the active learning sampling strategy repository we used is the Parzen Window Classifier (PWC). It is a non-parametric method used for classification and density estimation in machine learning. It works by estimating the probability density function of a given class using a kernel density estimator, and then using Bayes' theorem to classify new instances based on their estimated probability densities.

We will also explore using other classifiers from Scikit-Learn and TensorFlow and compare their performance on the data without using active learning to see if there is any improvement beyond the PWC classifier.

\section{Summary}

It should now be more clear how the sampling strategy is the major component of active learning. The query function construction is also important but it is just a means of routing the data to be sampled. In the next chapter we will look into the specifics of xPAL.
