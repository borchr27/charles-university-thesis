\chapter*{Definitions}
\addcontentsline{toc}{chapter}{Definitions}

In this section we define some terms and ideas that will be helpful in understanding the upcoming sections.

\begin{defn}[Beta Prior]
\label{def:beta_prior}
A beta prior is a conjugate prior for the binomial distribution. It is a continuous probability distribution defined on the interval [0, 1] and is parameterized by two positive shape parameters, \(\alpha\) and \(\beta\). The beta distribution is defined as: 
\[\text{Beta}(\alpha, \beta) = \frac{\Gamma(\alpha + \beta)}{\Gamma(\alpha)\Gamma(\beta)}x^{\alpha - 1}(1 - x)^{\beta - 1}\]
where \(\Gamma\) is the gamma function and \(x\) is a random variable. The gamma function is defined as:
\[\Gamma(x) = \int_0^\infty t^{x - 1}e^{-t}dt\]
The gamma function is used as a normalizing constant to ensure that the probability density function integrates to 1 over the simplex, which is the space of all probability vectors that sum to 1.
\end{defn}

\begin{defn}[Conjugate Prior]
\label{def:conjugate_prior}
A conjugate prior is a prior distribution that is in the same family of distributions as the likelihood function. In other words, the posterior distribution will have a similar functional form to the prior distribution.
\end{defn}

\begin{defn}[Decision-Theoretic]
\label{def:decision_theoretic}
Decision-theoretic active learning is a framework that uses the expected performance gain of a candidate to determine which candidate to label. The expected performance gain is the expected performance of the classifier after labeling the candidate minus the expected performance of the classifier before labeling the candidate. The expected performance of the classifier is the expected value of the performance measure given the posterior distribution of the classifier.
\end{defn}

\begin{defn}[Dirichlet Distribution]
\label{def:dirichlet_distribution}
The Dirichlet distribution is a multivariate generalization of the beta distribution. It is a continuous probability distribution defined on the \(K\)-simplex, \(\Delta_K = \{x \in \mathbb{R}^K: x_i \geq 0, \sum_{i=1}^K x_i = 1\}\). The Dirichlet distribution is parameterized by a vector of positive shape parameters, \(\alpha = (\alpha_1, \alpha_2, \dots, \alpha_K)\). The Dirichlet distribution is defined as:
\[\text{Dir}(\alpha) = \frac{\Gamma(\sum_{i=1}^K \alpha_i)}{\prod_{i=1}^K \Gamma(\alpha_i)}\prod_{i=1}^K x_i^{\alpha_i - 1}\]
where \(\Gamma\) is the gamma function as defined in Definition \ref{def:beta_prior} and where \(x\) is a random vector.
\end{defn}

\begin{defn}[Ground Truth]
\label{def:ground_truth}
Ground truth is the true label of a data point.
\end{defn}

\begin{defn}[Posterior Probabilities]
\label{def:posterior_probabilities}
Posterior probability is a type of conditional probability that results from updating the prior probability with information summarized by the likelihood via an application of Bayes' rule. The posterior probability is the probability of an event occurring given that another event has occurred.
\end{defn}

\begin{defn}[Omniscient Oracles]
\label{def:omniscient_oracles}
Omniscient oracle is a hypothetical entity that has complete knowledge of the true labels of all data points in a given dataset. An omniscient oracle knows the ground truth labels of all data points.
\end{defn}

\begin{defn}[TF-IDF]
\label{def:tf_idf}
TF-IDF is a numerical statistic that is intended to reflect how important a word is to a document in a collection or corpus. It is often used as a weighting factor in information retrieval and text mining. The TF-IDF value increases proportionally to the number of times a word appears in the document and is offset by the frequency of the word in the corpus (large structured set or collection of speech or text data).
\end{defn}