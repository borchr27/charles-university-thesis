\chapter*{Conclusion}
\addcontentsline{toc}{chapter}{Conclusion}

Our goal was to understand the entire process including the web scraping, translating, storing, and performance of the selection strategies and classifiers. This analysis provides some insight for our partner and allows them to learn from our tests and experiments. 

The scraping and data collection process was an exercise in itself. We initially wanted to dockerize the entire project but this was more time consuming and cumbersome than expected, so this approach was abandoned. We setup Scrapy to take a list of websites as input and then navigate the main webpage and scrape the html. We then processed the html and saved the text locally in a Postgres database. 

After the text was collected and stored we realized we had some issues because we assumed more of the text would be in English. At this stage, after inspecting the data we realized we had 9 categories with 2 or less samples and a total of 275 data points for 23 categories. However, we had a large amount of unused data that we needed to figure out how to use. We experimented with a number of API's and other tools for collecting English text but ultimately ended up using the Azure API for translation, which allowed us to have a bit larger data set of text to work with. 

One issue we struggled with was the quality of the collected data. We used some statistical methods to analyze the most frequent words and sometimes found non english words as being influential for a category. This emphasized a few different things. First, was that maybe scraping text data from just a websites homepage isn't enough and experiments should be run with additional pages from the site tree. Second, that we may be lacking in quality train data as a result of our raw text scrubbing and preprocessing methods were not optimal. Finally, we could have explored better web scraping tools to collect more data from the websites. However, at a certain point the scraper was performing well enough and we decided to move on to keep with our timeline.

Our next task was to start experimenting with Scikit-Learn and TensorFlow classifiers and see how they performed on our data. We used a number of different classifiers and found that the linear support vector classifier performed well with the data. However, we only scratched the surface with TensorFlow and more testing could be done with it. 

We also used the Parzen Window Classifier from the Probabilistic Active Learning GitHub repository from the "Toward optimal probabilistic active learning using a Bayesian approach" paper by \cite{kottke2021toward}. This repository provided a number of different sampling strategies and we modified the repository to fit our needs and data. We found that the xPAL sampling strategy was the best for our data and was able to reduce the testing error the most compared to the other available sampling strategies in the repository based on our results.

Our work is collected into two repositories. One repository is the main repository that contains the code for the web scraper, data processing, and the thesis. The second repository is a fork of the Probabilistic Active Learning from \cite{kottke2021toward} repository that contains the code for the Parzen Window Classifier, xPAL, and other sampling strategies as well as the main active learning experiments, results, vectorized data from previous experiments, and text data. The raw text data scraped from the websites is stored locally. The main repository can be found at \url{https://github.com/borchr27/charles-university-thesis} and the Probabilistic Active Learning repository is available at \url{https://github.com/borchr27/probal}.

The Probabilist Active Learning repository was setup with the PWC so that when new data was added the classifier could be updated quickly and only where there was a change or there was data effected by the new data. This caused issues as we tried to implement LinearSVC with xPAL as we would naively have to retrain the entire model for every new data point.

To improve performance we would suggest making a number of changes. One of the first changes would be to try and improve the quality of the text data from the websites. For example, a stronger web scraper could have allowed us to avoid potential IP address restriction issues and scrape data from social media websites that refused to allow our scraper to collect any data and resulted in some unusable data. We could have also run our own tests regarding how the number of sibling pages could have fortified the data.

The scraper could be upgraded to scrape social media pages. This could improve the quality of the data and allow for more accurate classification. We think that improving the quality of the data is one of the key points to improving the performance.

% It also seems that a small amount of good text data is better than a large amount of poor quality text data. Another way we could have made the scraped data better for the classifier is to take out all non english words (post translation).

To be more thorough, we could have run exhaustive tests for \textit{all} the available classifiers within Scikit-Learn using GridSearchCV and other ensemble methods to see if any more performance gains were attainable. 

The current setup of the active learning xPAL process uses the PWC as it is fast (because it updates only parts of the classifier with each new data point) and relatively simple to implement. However, it is not the best method for classifying our data as we have seen from our experiments. Based on our experiments we assume that we could improve the performance of the classifier by implementing xPAL with the LinearSVC classifier, but we weren't able to test this explicitly. We discussed this with the authors of the Probabilistic Active Learning repository \cite{kottke2021toward} and it could be the focus of future work.

Although there are many things that could have been improved, we still were able to increase our understanding about the data collection process, translation, storage, classification, active learning, and the nature of the data itself. 

In conclusion, we found that xPAL appeared to select the best data samples to train a classifier earlier and faster than other active learning methods and we found that the LinearSVC classifier performed best in comparison to the other classifiers we tested.
