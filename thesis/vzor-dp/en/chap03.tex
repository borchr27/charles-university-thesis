\chapter{Analysis}

In this section we take a deeper look into the data, the process of augmenting the data, the experiment, and the results. Our partner has provided a small sample of 1000 labeled data points. This data was manually labeled by an annotator. The data consists of a merchant name, merchant website (url), merchant category, and merchant tag as shown in Table \ref{tab:data_point}. 


\begin{table}[h]
\begin{tabular}{|l|l|l|l|}
\hline
merchant name            & merchant url            & merchant category & merchant tags           \\ \hline
State Hospital & http://hospital.com/ & Health   & '\{"Clinic"\}' \\ \hline
\end{tabular}
\caption{This is an example of a single data point.}
\label{tab:data_point}
\end{table}


The current process consists of giving the merchant url to an annotator and the annotator then views the website and either can instantly  provide a label and tags for the website or in some cases may need to browse further into the website (by viewing sibling pages such as the 'About Us' sections or product pages) to get an idea of how the website should be classified. 

The merchant tags are ordered by relevance, with the first tag in the list being the most general and the final being the most specific. An example of the tag hierarchy is show in Table. \ref{tab:tags} where we can see that this sample consists of data from various categories all contained within the 'Eco' side tag grouping.


\begin{table}[h]
\begin{tabular}{|l|l|l|l|l|}
\hline
Category       & Level 1                     & Level 2                       & Level 3      & Side Tag \\ \hline
Travel         & Local Transport             & Micro-mobility                & Bike Sharing & Eco      \\ \hline
               &                             & Public Transport              &              & Eco      \\ \hline
Fashion        & Clothing - Other            & Second Hand                   &              & Eco      \\ \hline
Car            & Charging Station            &                               &              & Eco      \\ \hline
               & Car Sharing                 &                               &              & Eco      \\ \hline
\end{tabular}
\caption{This is an example of how the tags use different levels.}
\label{tab:tags}
\end{table}


Similarly to the annotator our goal is to automate the navigation, collection/storing process, and classification of the website. This pipeline speeds up the browsing process and can allow the annotator to spend much less time annotating and require the annotator to only anotate data expected to drastically improve the classifier.

The initial 1000 data points we received were essentially just labels with a pointer (a url) to where the data is. The labels needed the text from the websites that the annotator viewed to begin the classifing process. To gather the text data from the websites we used the Scrapy framework to extract text data from a single top level page of a website. We chose to only scrape the top level page text because
of the results published in another study where it was observed that adding more pages to the dataset does not necessiarily mean obtaining better resutls (\cite{sahid2019ecommerce}). 

Out of these initial data points 179 contained links that could not be accessed or links that provided no text data that could be scraped. Out of the remaining 821 data points 274 of them were in English. 

It is important for us to have the data in Englsih as it allows us to exploit stop words when using the Scikit-Learn TF-IDF Vectorizer to construct our dataset. Stop words are words like “and”, “the”, “him”, which are presumed to be uninformative in representing the content of a text, and which may be removed to avoid them being construed as signal for prediction (\cite{sklearn62feature}).

Out of the remaining 275 english data points the data was distributed into the categories as shown in Figure \ref{fig:og_en_hist}.

\begin{figure}[!ht]
  \centering
  \includegraphics[width=\textwidth]{../img/plot_og_en_hist.pdf}
  \caption{The histograms for the original usable english data.}
  \label{fig:og_en_hist}
\end{figure}

At this stage, it was clear that our dataset wasnt representing all cataegories equally. The 'Food and Drink' category has many more data points then the 'Culture' and 'Investments' categories which each had a single data point. The 'Children' and 'Financial Services' categories weren't represented at all. Obviously this was problamatic becuase we would like to have, minium, three data points in each category to build train, test, and validation datasets, albeit limited as this would be.

At this point we had a significant amount of data that wasnt being used. We decided to find a way to translate the existing data. We tried various librarie available on GitHub but we weren't getting good results. We found that Azure had a service available and a free option of up to 2 million characters translated per month. This was a perfect option and we were able to use this api to translate the remaining data.

In addition to the translated data we also manually collected and labeled 141 additional data points to use mainly for validation. All the original data and additional data are shown in Figure \ref{fig:all_hist}

\begin{figure}[!ht]
  \centering
  \includegraphics[width=\textwidth]{../img/plot_all_hist.pdf}
  \caption{The histograms for the original and additional data for all languages.}
  \label{fig:all_hist}
\end{figure}


In addition to translating the data we used the TF-IDF vectorizer from Scikit-Learn. With just the original data we were able to find the highly correlated words for each category, shown below in Table \ref{tab:correlated_unigrams_original}. Some catgegories such as 'Culture', 'Digital Services', 'Shopping Online' that have few data points have words such as 'kihnu', 'td', 'patria', respectively, have no relative meaning to the category in English. This is likely because the collected data is sparse and the quality of the scraped text data is low.



\begin{table}[!ht]
\centering
\caption{Keywords from TF-IDF with Chi Squared using the original data.}
\input{../img/table_correlated_unigrams_original.tex}
\label{tab:correlated_unigrams_original}
\end{table}










