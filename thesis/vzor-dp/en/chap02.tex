\chapter{Defining xPAL}

We have introduced many different active learning sampling strategies in the previous section, and we will use them to test which strategy performs best with our data. However, we will mainly focus on using the xPAL sampling strategy and a pool based query function. The xPAL sampling strategy is a decision-theoretic approach to measure the usefulness of a labeling candidate in terms of its expected performance gain (\cite{kottke2021toward}). We can estimate the data distribution but we are uncertain about the true class posterior probabilities. The class posterior probabilities are modeled as a random variable based on the current observations. Therefore a Bayesian approach is used by incorporating a conjugate prior to the observations. In general, the idea is to estimate the expected performance gain for the classifier, using the unlabeled data, and then select the best data point and request or reveal its label. Descriptions of the variables used throughout the paper are listed in Table \ref{tab:var_defs}.

\begin{table}[ht]
\centering
\renewcommand{\arraystretch}{1.2}
\begin{tabular}{|l|l|}
\hline
{} & \textbf{Descriptions} \\
\hline
$C$                         & Number of classes \\
\hline
$x$                         & Input $x \in \mathbb{R}^D$ (D-dimensional vector space)\\
\hline
$y$                         & Class label $y \in \mathcal{Y}$ \\
\hline
$\mathcal{Y}$               & Set of all labels $\mathcal{Y} = \{ 1,...,C \}$ \\
\hline
$f^{\mathcal{L}}$           & Classifier that maps input $x$ to label $y$ using $\mathcal{L}$ \\
\hline
$\textit{L}$                & Loss\\
\hline
$\textit{R}$                & Risk $\textit{R}(f^{\mathcal{L}}) \in \mathbb{R}_0^x$ \\
\hline
$\textit{R}_{\mathcal{E}}$  & Empirical risk\\
\hline
$\mathcal{L}$               & Set of labeled data $\{(x_1,y_1),...,(x_n,y_n)\}$ \\
\hline
$\mathcal{U}$               & Set of unlabeled data $\{x_1,...,x_n\}$ \\
\hline
$\mathcal{E}$               & Set of available labeled and unlabeled data $\{x : (x,y) \in \mathcal{L}\} \cup \mathcal{U}$ \\
\hline
\end{tabular}
\caption{Variable names and descriptions.}
\label{tab:var_defs}
\end{table}

\section{Kernel}

A kernel based classifier is used in xPAL which determines the similarity of two data points. The kernel function $\textbf{\textit{K}}(x,x')$ is a function that maps two data points to a real number, which is then used to estimate the probability density of the data. The kernel frequency estimate $\textbf{\textit{k}}^{\mathcal{L}}_x$ of an instance $x$ is calculated using the labeled instances $\mathcal{L}$. The y-th element of that C-dimensional vector describes the similarity-weighted number of labels of class $y$.

\begin{equation}
\textbf{\textit{k}}^{\mathcal{L}}_x,y = \sum_{(x',y') \in \mathcal{L}} \mathbb{1}_{y=y'} \textbf{\textit{K}}(x,x')
\label{eq:kernel}
\end{equation}

The Parzen Window Classifier uses the labeled data for training and predicts the most frequent class and was selected by Kottke et al. to use because of its speed and ability to implement different kernels depending on the data (\cite{kottke2021toward}). It was used for all the selection strategies in their experiments. 

\begin{equation}
f^{\mathcal{L}}(x) = \underset{y \in \mathcal{Y}}{\arg \max} \left( \textbf{\textit{k}}^{\mathcal{L}}_{x,y} \right) \\
\end{equation}

We will use the PWC classifier for our experiments because that is what is implemented with the active learning strategies, but we will also evaluate other classifiers and compare their performance less active learning.

\section{Risk}

For xPAL, Kottke et al. use the classification error as the performance measure and minimize the zero-one loss. The risk describes the expected value of the loss relative to the joint distribution given some classifier. The zero-one loss returns 0 if the prediction from the classifier is equal to the true class else it returns 1. The risk is a theoretical concept that cannot be computed directly since it requires knowledge of the entire population distribution. Instead, we typically try to approximate the risk using the empirical risk.

\begin{flalign}
\textit{R}(f^{\mathcal{L}}) &= \underset{p(x,y)}{\mathbb{E}} [ \textbf{\textit{L}}(y,f^{\mathcal{L}}(x)) ] \\
&= \underset{p(x)}{\mathbb{E}} \left[ \underset{p(y|x)}{\mathbb{E}} [ \textbf{\textit{L}}(y,f^{\mathcal{L}}(x)) ] \right] \\
\textbf{\textit{L}}(y,f^{\mathcal{L}}(x)) &= \mathbb{1}_{f^{\mathcal{L}}(x)\neq y} 
\end{flalign}

Because it is not known how the data is generated Kottke et al. use a Monte-Carlo integration with all available data $\mathcal{E}$ to represent the generator. The empirical risk $\textit{R}_{\mathcal{E}}$ is the average of the loss over all data in the dataset. It refers to the average value of a given loss function over a finite set of observed data points. 

\begin{flalign}
    \textit{R}_{\mathcal{E}}(f^{\mathcal{L}}) &= \frac{1}{|\mathcal{E}|} \sum_{x \in \mathcal{E}} \underset{p(y|x)}{\mathbb{E}} \left[ \textbf{\textit{L}}(y,f^{\mathcal{L}}(x)) \right] \\
    &= \frac{1}{|\mathcal{E}|} \sum_{x \in \mathcal{E}} \sum_{y \in \mathcal{Y}} p(y|x) \textbf{\textit{L}}(y,f^{\mathcal{L}}(x))
\label{eq:empirical_risk}
\end{flalign}

The empirical risk is a computable quantity that can be used as an estimate of the risk. However, it is only an approximation and is subject to sampling error.


\section{Conjugate Prior}

The conditional class probability $p(y|x)$ depends on the ground truth which is unknown. As a result the conditional class probability is exactly the y-th element of the unknown ground truth vector $\textit{\textbf{p}}$. The nearby labels from $\mathcal{L}$ can be used to estimate the ground truth $\textit{\textbf{p}}$ because the oracle provides the labels according to $\textit{\textbf{p}}$. If we assume a smooth distribution then the estimate is relatively close to the ground truth if we have enough labeled instances. 

\begin{equation}
p(y|x) = p(y|t(x)) = p(y|\textit{\textbf{p}}) = \text{Cat} (y|\textit{\textbf{p}}) = p_y
\end{equation}

A Bayesian approach is used for estimation by calculating the posterior predictive distribution (calculating the expected value over all possible ground truth values). The probability of $y$ given some $x$ is approximately equal to the kernel frequency estimate of $x$. 

\begin{equation}
p(y|x) \approx  p(y | \textbf{\textit{k}}^{\mathcal{L}}_x) = \underset{p(\textit{\textbf{p}}|\textbf{\textit{k}}^{\mathcal{L}}_x)}{\mathbb{E}} \left[ p_y \right] = \int p(\textit{\textbf{p}}|\textbf{\textit{k}}^{\mathcal{L}}_x) p_y d \textit{\textbf{p}}
\label{eq:kernel_estimate}
\end{equation}

Bayes theorem is then used to determine the posterior probability of the ground truth at instance $x$ in Equation \ref{eq:bayes}. The likelihood $p(\textbf{\textit{k}}^{\mathcal{L}}_x|p)$ is a multinomial distribution because each label has been drawn from $\text{Cat}(y|\textit{\textbf{p}})$. A prior is introduced and selected as a Dirichlet distribution with $\alpha \in \mathbb{R}^C$ as this is the conjugate prior of the multinomial distribution. An indifferent prior is chosen and each element of alpha is set to the same value. The Dirichlet distribution is an analytical solution for the posterior when the conjugate prior of the multinomial likelihood are used. 

\begin{flalign}
\label{eq:bayes}
p(\textit{\textbf{p}}|\textbf{\textit{k}}^{\mathcal{L}}_x) &= \frac{p(\textbf{\textit{k}}^{\mathcal{L}}_x|\textit{\textbf{p}})p(\textit{\textbf{p}})}{p(\textbf{\textit{k}}^{\mathcal{L}}_x)} \\
&= \frac{\text{Mult}(\textbf{\textit{k}}^{\mathcal{L}}_x|\textit{\textbf{p}}) \cdot \text{Dir}(\textit{\textbf{p}}|\alpha)}{\int \text{Mult}(\textbf{\textit{k}}^{\mathcal{L}}_x|\textit{\textbf{p}}) \cdot \text{Dir}(\textit{\textbf{p}}|\alpha) d\textit{\textbf{p}}} \\
&= \text{Dir}(\textit{\textbf{p}}|\textbf{\textit{k}}^{\mathcal{L}}_x + \alpha)
\end{flalign}

The conditional class probability is determined next from Equation \ref{eq:kernel_estimate}. It is calculated with the expected value of the Dirichlet distribution.

\begin{flalign}
p(y|\textbf{\textit{k}}^{\mathcal{L}}_x) &= \underset{\text{Dir}(\textit{\textbf{p}}|\textbf{\textit{k}}^{\mathcal{L}}_x+ \alpha)}{\mathbb{E}} \left[ p_y \right] \\
&= \int \text{Dir}(\textit{\textbf{p}}|\textbf{\textit{k}}^{\mathcal{L}}_x + \alpha) p_y d\textit{\textbf{p}} \\
&= \frac{\textbf{(\textit{k}}^{\mathcal{L}}_x + \alpha)_y}{||\textbf{\textit{k}}^{\mathcal{L}}_x + \alpha ||_1}
\label{eq:cond_class_prob}
\end{flalign}

The last term is the y-th element of the normalized vector. The 1-norm is used to normalize the vector.

\section{Risk Difference Using the Conjugate Prior}

Next, we insert equation \ref{eq:cond_class_prob} into the empirical risk equation \ref{eq:empirical_risk}. We are approximating $p(y|x)$ with $p(y|\textbf{\textit{k}}_x^\mathcal{L})$ which is the empirical risk based on the labeled data $\mathcal{L}$.

\begin{equation}
    \hat{R}_{\mathcal{E}} (f^{\mathcal{L}}, \mathcal{L}) = \frac{1}{|\mathcal{E}|} \sum_{x \in \mathcal{E}} \sum_{y \in \mathcal{Y}}  \frac{\textbf{(\textit{k}}^{\mathcal{L}}_x + \alpha)_y}{||\textbf{\textit{k}}^{\mathcal{L}}_x + \alpha ||_1} \cdot \textit{L} (y,f^{\mathcal{L}}(x))
\end{equation}

Now lets assume we add a new labeled candidate $(x_c,y_c)$ to the labeled data set $\mathcal{L}$. We will now denote the set with the newly labeled data point $\mathcal{L}^+ = \mathcal{L} \cup \{(x_c,y_c)\}$. Next we need to determine how much this new data point improved our classifier. We then make an estimate of the gain in terms of risk difference using the probability to estimate the ground truth.

\begin{flalign}
    \Delta \hat{R}_{\mathcal{E}} (f^{\mathcal{L}^+}, f^{\mathcal{L}}, \mathcal{L}^+) =& \hat{R}_{\mathcal{E}} (f^{\mathcal{L}^+}, \mathcal{L}^+) - \hat{R}_{\mathcal{E}} (f^{\mathcal{L}}, \mathcal{L}^+) \\
    =& \frac{1}{|\mathcal{E}|} \sum_{x \in \mathcal{E}} \sum_{y \in \mathcal{Y}}  \frac{\textbf{(\textit{k}}^{\mathcal{L}^+}_x + \alpha)_y}{||\textbf{\textit{k}}^{\mathcal{L}^+}_x + \alpha ||_1} \\
    &\cdot \left( \textit{L} (y,f^{\mathcal{L}^+}(x)) - \textit{L}(y,f^{\mathcal{L}}(x)) \right)
\label{eq:delta_emp_risk}
\end{flalign}

The observations used to estimate the risk are the same for both the old and new classifiers. We do this because we assume that adding labeled data will make the classifier better, so this allows us to more accurately compare the current classifier and the new one.

\section{Expected Probabilistic Gain}

If we are able to reduce the error with the new $\mathcal{L}^+$ model then equation \ref{eq:delta_emp_risk} will be negative. As a result, we negate this term and maximize the expected probabilistic gain. To simplify things we set $\alpha = \beta$.

\begin{flalign}
    \text{xgain}(x_c, \mathcal{L}, \mathcal{E}) =& \underset{p(y_c|\textbf{\textit{k}}^{\mathcal{L}}_{x_c})}{\mathbb{E}} \left[ - \Delta \hat{R}_{\mathcal{E}} (f^{\mathcal{L}^+}, f^{\mathcal{L}}, \mathcal{L}^+ \right] \\
    =& - \sum_{y \in \mathcal{Y}}  \frac{\textbf{(\textit{k}}^{\mathcal{L}}_x + \beta)_y}{||\textbf{\textit{k}}^{\mathcal{L}}_x + \beta ||_1} \cdot \frac{1}{|\mathcal{E}|} \sum_{x \in \mathcal{E}} \sum_{y \in \mathcal{Y}} \notag \\
    & \frac{\textbf{(\textit{k}}^{\mathcal{L}^+}_x + \alpha)_y}{||\textbf{\textit{k}}^{\mathcal{L}^+}_x + \alpha ||_1} \cdot \left( \textbf{\textit{L}} (y,f^{\mathcal{L}^+}(x)) - \textbf{\textit{L}}(y,f^{\mathcal{L}}(x)) \right)
\end{flalign}

Finally, for the xPAL selection strategy, we simply choose this candidate $x^*_c \in \mathcal{U}$ where the gain is maximized: 

\begin{equation}
    x^*_c = \underset{x_c \in \mathcal{U}}{\arg \max} \left( \text{xgain} (x_c, \mathcal{L}, \mathcal{E}) \right) \hbox{.}
\end{equation}
