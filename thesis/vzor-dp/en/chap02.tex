\chapter{Understanding xPAL}

We have introduced many different active learning models in the previous section, and we will test some of these models on our data. However, we will mainly focus on using the xPAL sampling strategy and a pool based query function. The xPAL sampling strategy is a decision-theoretic approach to measure the usefulness of a labeling candidate in terms of its expected performance gain (\cite{kottke2021toward}). We can estimate the data distribution but we are uncertain about the true class posterior probabilities. The class posterior probabilities are modeled as a random variable based on the current observations. Therefore a Bayesian approach is used by incorporating a conjugate prior to the observations. In general, the idea is to estimate the expected performance gain for the classifier, using the unlabeled data, and then select the best data point and request its label. Variable descriptions used in the following equations and explanations are listed in Table \ref{tab:var_defs}

\begin{table}[ht]
\centering
\begin{tabular}{|l|l|}
\hline
{} & \textbf{Descriptions} \\
\hline
$C$                         & Number of classes \\
\hline
$x$                         & Input $x \in \mathbb{R}^n$\\
\hline
$y$                         & Output $y \in \{ l_1,...,l_C \}$ \\
\hline
$\textit{L}$                & Loss\\
\hline
$\textit{R}$                & Risk $\textit{R}(f^{\mathcal{L}}) \in \mathbb{R}_0^x$ \\
\hline
$\textit{R}_{\mathcal{E}}$  & Empirical risk\\
\hline
$\mathcal{L}$               & Set of labeled data $\{(x_1,y_1),...,(x_n,y_n)\}$ \\
\hline
$\mathcal{U}$               & Set of unlabeled data $\{x_1,...,x_n\}$ \\
\hline
$\mathcal{E}$               & Set of labeled and unlabeled features $\{\textbf{\textit{x}}\}$ \\
\hline
$p(x,y)$                    & Joint distribution of random variables $x$ and $y$ \\
\hline
$f^{\mathcal{L}}$           & Classifier that maps input $x$ to output $y$ \\
\hline
\end{tabular}
\caption{Variable names and definitions.}
\label{tab:var_defs}
\end{table}

\section{Kernel}

A kernel based classifier is used in xPAL which determines the similarity of two data points. The kernel function $\textbf{\textit{K}}(x,x')$ is a function that maps two data points to a real number. The kernel frequency estimate $\textbf{\textit{k}}^{\mathcal{L}}_x$ of an instance $\textbf{\textit{x}}$ is calculated using the labeled instances $\mathcal{L}$. The y-th element of that C-dimensional vector describes the similarity-weighted number of labels of class y.

\begin{equation}
\textbf{\textit{k}}^{\mathcal{L}}_x,y = \sum_{(x',y') \in \mathcal{L}} \mathbb{1}_{y=y'} \textbf{\textit{K}}(x,x')
\label{eq:kernel}
\end{equation}

The Parzen Window Classifier uses the labeled data for training and predicts the most frequent class and was selected by Kottke et al. to use because of its speed and ability to implement different kernels depending on the data (\cite{kottke2021toward}). It was used for all the selection strategies in their experiments. 

\begin{equation}
f^{\mathcal{L}}(x) = \underset{y \in \mathcal{Y}}{\arg \max} \left( \textbf{\textit{k}}^{\mathcal{L}}_{x,y} \right) \hbox{.}
\end{equation}

We will mainly use the PWC classifier in our experiments but we will also evaluate other classifiers and compare their performance.

\section{Risk}

For xPAL, Kottke et al. use the classifications error as the performance measure and minimize the zero-one loss. The risk describes the expected value of the loss relative to the joint distribution given some classifier. The zero-one loss returns 0 if the prediction from the classifier is equal to the true class else it returns 1. The risk is a theoretical concept that cannot be computed directly since it requires knowledge of the entire population distribution. Instead, we typically try to approximate the risk using the empirical risk.

\begin{flalign}
\textit{R}(f^{\mathcal{L}}) &= \underset{p(x,y)}{\mathbb{E}} [ \textbf{\textit{L}}(y,f^{\mathcal{L}}(x)) ] \\
&= \underset{p(x)}{\mathbb{E}} \left[ \underset{p(y|x)}{\mathbb{E}} [ \textbf{\textit{L}}(y,f^{\mathcal{L}}(x)) ] \right] \\
\textbf{\textit{L}}(y,f^{\mathcal{L}}(x)) &= \mathbb{1}_{f^{\mathcal{L}}(x)\neq y} 
\end{flalign}

Because it is not known how the data is generated Kottke et al. use a Monte-Carlo integration with all the data $\mathcal{E}$ to represent the generator. The empirical risk $\textit{R}_{\mathcal{E}}$ is the average of the loss over all the data points in the dataset. It refers to the average value of a given loss function over a finite set of observed data points. The empirical risk is a computable quantity that can be used as an estimate of the risk. However, it is only an approximation and is subject to sampling error.

\begin{flalign}
    \textit{R}_{\mathcal{E}}(f^{\mathcal{L}}) &= \frac{1}{|\mathcal{E}|} \sum_{x \in \mathcal{E}} \underset{p(y|x)}{\mathbb{E}} \left[ \textbf{\textit{L}}(y,f^{\mathcal{L}}(x)) \right] \\
    &= \frac{1}{|\mathcal{E}|} \sum_{x \in \mathcal{E}} \sum_{y \in \mathcal{Y}} p(y|x) \textbf{\textit{L}}(y,f^{\mathcal{L}}(x))
\label{eq:empirical_risk}
\end{flalign}


\section{Conjugate Prior}

The conditional class probability $p(y|x)$ depends on the ground truth which is unkown. As a result the conditional class probability is exactly the y-th element of the unknown ground truth vector $\textit{\textbf{p}}$. The nearby labels from $\mathcal{L}$ can be used to estimate the ground truth $\textit{\textbf{p}}$ becuase the oracle provides the labels according to $\textit{p}$. If we assume a smooth distribution then the estimate is relatively close to the ground truth if we have enough labeled instances. 

\begin{equation}
p(y|x) = p(y|t(x)) = p(y|\textit{\textbf{p}}) = \text{Cat} (y|\textbf{p}) = p_y
\end{equation}

A Bayesian approach is used for estimation by calculating the posterior predictive distribution (calculating the expected value over all possible ground truth values). The probability of y given some x is approximately equal to the kernel frequency estimate of x. 

\begin{equation}
p(y|x) \approx  p(y | \textbf{\textit{k}}^{\mathcal{L}}_x) = \underset{p(p|\textbf{\textit{k}}^{\mathcal{L}}_x)}{\mathbb{E}} \left[ p_y \right] = \int p(p|\textbf{\textit{k}}^{\mathcal{L}}_x) p_y dp
\label{eq:kernel_estimate}
\end{equation}

Bayes theorem is then used to determine the posterior probability of the ground truth at instance x in Equation \ref{eq:bayes}. The likelihood $p(\textbf{\textit{k}}^{\mathcal{L}}_x|p)$ is a multinomial distribution because each label has been drawn from $Cat(y|p)$. A prior is introduced and selected as a Dirichlet distribution with $\alpha \in \mathbb{R}^C$ as this is the conjugate prior of the multinomial distribution. An indifferent prior is choosen and each element of alpha is set to the same value. The Dirichlet distribution is an analytical solution for the posterior when the conjugate prior of the multinomial likelihood are used. 

\begin{flalign}
\label{eq:bayes}
p(p|\textbf{\textit{k}}^{\mathcal{L}}_x) &= \frac{p(\textbf{\textit{k}}^{\mathcal{L}}_x|p)p(p)}{p(\textbf{\textit{k}}^{\mathcal{L}}_x)} \\
&= \frac{\text{Mult}(\textbf{\textit{k}}^{\mathcal{L}}_x|p) \cdot \text{Dir}(p|\alpha)}{\int \text{Mult}(\textbf{\textit{k}}^{\mathcal{L}}_x|p) \cdot \text{Dir}(p|\alpha) dp} \\
&= \text{Dir}(p|\textbf{\textit{k}}^{\mathcal{L}}_x + \alpha)
\end{flalign}

The conditional class probability is determined next from Equation \ref{eq:kernel_estimate}. It is calculated with the expected value of the Dirichlet distribution.

\begin{flalign}
p(y|\textbf{\textit{k}}^{\mathcal{L}}_x) &= \underset{\text{Dir}(p|\textbf{\textit{k}}^{\mathcal{L}}_x+ \alpha)}{\mathbb{E}} \left[ p_y \right] \\
&= \int \text{Dir}(p|\textbf{\textit{k}}^{\mathcal{L}}_x + \alpha) p_y dp \\
&= \frac{\textbf{(\textit{k}}^{\mathcal{L}}_x + \alpha)_y}{||\textbf{\textit{k}}^{\mathcal{L}}_x + \alpha ||_1}
\label{eq:cond_class_prob}
\end{flalign}

The last term is the y-th element of the normalized vector. The 1-norm is used to normalize the vector.

\section{Risk Difference Using the Conjugate Prior}

Next, we insert equation \ref{eq:cond_class_prob} into the empirical risk equation \ref{eq:empirical_risk}. We are approximating $p(y|x)$ with $p(y|\textbf{\textit{k}}_x^\mathcal{L})$ which is the empirical risk based on the labeled data $\mathcal{L}$.

\begin{equation}
    \hat{R}_{\mathcal{E}} (f^{\mathcal{L}}, \mathcal{L}) = \frac{1}{|\mathcal{E}|} \sum_{x \in \mathcal{E}} \sum_{y \in \mathcal{Y}}  \frac{\textbf{(\textit{k}}^{\mathcal{L}}_x + \alpha)_y}{||\textbf{\textit{k}}^{\mathcal{L}}_x + \alpha ||_1} \cdot \textit{L} (y,f^{\mathcal{L}}(x))
\end{equation}

Now lets assume we add a new labeled candidate $(x_c,y_c)$ to the labeled data set $\mathcal{L}$. We will now denote the set with the newly labeled data point $\mathcal{L}^+ = \mathcal{L} \cup \{(x_c,y_c)\}$. Next we need to determine how much this new data point improved our classifier. We then make an estimate of the gain in terms of risk difference using the probability to estimate the ground truth.

\begin{flalign}
    \Delta \hat{R}_{\mathcal{E}} (f^{\mathcal{L}^+}, f^{\mathcal{L}}, \mathcal{L}^+) &= \hat{R}_{\mathcal{E}} (f^{\mathcal{L}^+}, \mathcal{L}^+) - \hat{R}_{\mathcal{E}} (f^{\mathcal{L}}, \mathcal{L}^+) \\
    &= \frac{1}{|\mathcal{E}|} \sum_{x \in \mathcal{E}} \sum_{y \in \mathcal{Y}}  \frac{\textbf{(\textit{k}}^{\mathcal{L}^+}_x + \alpha)_y}{||\textbf{\textit{k}}^{\mathcal{L}^+}_x + \alpha ||_1} \cdot \left( \textit{L} (y,f^{\mathcal{L}^+}(x)) - \textit{L}(y,f^{\mathcal{L}}(x)) \right)
\label{eq:delta_emp_risk}
\end{flalign}

The observations used to estimate the risk are the same for both the old and new classifiers. We do this because we assume that adding labeled data will make the classifier better, so this allows us to more accurately compare the current classifier and the new one.

\section{Expected Probabilistic Gain}

If we are able to reduce the error with the new $\mathcal{L}^+$ model then equation \ref{eq:delta_emp_risk} will be negative. As a result, we negate this term and maximize the expected probabilistic gain. To simplify we set $\alpha = \beta$.

\begin{flalign}
    \text{xgain}(x_c, \mathcal{L}, \mathcal{E}) =& \underset{p(y_c|\textbf{\textit{k}}^{\mathcal{L}}_{x_c})}{\mathbb{E}} \left[ - \Delta \hat{R}_{\mathcal{E}} (f^{\mathcal{L}^+}, f^{\mathcal{L}}, \mathcal{L}^+ \right] \\
    =& - \sum_{y \in \mathcal{Y}}  \frac{\textbf{(\textit{k}}^{\mathcal{L}}_x + \beta)_y}{||\textbf{\textit{k}}^{\mathcal{L}}_x + \beta ||_1} \cdot \frac{1}{|\mathcal{E}|} \sum_{x \in \mathcal{E}} \sum_{y \in \mathcal{Y}} \notag \\
    & \frac{\textbf{(\textit{k}}^{\mathcal{L}^+}_x + \alpha)_y}{||\textbf{\textit{k}}^{\mathcal{L}^+}_x + \alpha ||_1} \cdot \left( \textbf{\textit{L}} (y,f^{\mathcal{L}^+}(x)) - \textbf{\textit{L}}(y,f^{\mathcal{L}}(x)) \right)
\end{flalign}

\section{xPAL Selection Strategy}

The xPAL selection strategy chooses this candidate $x^*_C \in \mathcal{U}$ where the gain is maximized:

\begin{equation}
    x^*_c = \underset{x_c \in \mathcal{U}}{\arg \max} \left( \text{xgain} (x_c, \mathcal{L}, \mathcal{E}) \right)
\end{equation}
