\chapter{Understanding xPAL}

We have introduced many different active learning models in the previous section, and we will test some of these models on our data. However, we will mainly focus on using the xPAL sampling strategy and a pool based query function. The xPAL sampling strategy is a decision-theoretic approach to measure the usefulness of a labeling candidate in terms of its expected performance gain (\cite{kottke2021toward}). We can estimate the data distribution but we are uncertain about the true class posterior probabilities. The class posterior probabilities are modeled as a random variable based on the current observations. Therefore a Bayesian approach is used by incorporating a conjugate prior to the observations. In general, the idea is to estimate the expected performance gain for the classifier, using the unlabeled data, and then select the best data point and request its label. Variable descriptions used in the following equations and explanations are listed in Table \ref{tab:var_defs}

\begin{table}[ht]
\centering
\begin{tabular}{|l|l|}
\hline
{} & \textbf{Descriptions} \\
\hline
$C$                         & Number of classes \\
\hline
$x$                         & Input $x \in \mathbb{R}^n$\\
\hline
$y$                         & Output $y \in l_1,...,l_C$ \\
\hline
$\textit{L}$                & Loss ??\\
\hline
$\textit{R}$                & Risk $\textit{R}(f^{\mathcal{L}}) \in \mathbb{R}_0^x$ \\
\hline
$\textit{R}_{\mathcal{E}}$  & Empirical risk ??\\
\hline
$\mathcal{L}$               & Set of labeled data $\{(x_1,y_1),...,(x_n,y_n)\}$ \\
\hline
$\mathcal{U}$               & Set of unlabeled data $\{x_1,...,x_n\}$ \\
\hline
$\mathcal{E}$               & Set of labeled and unlabeled data \\
\hline
$p(x,y)$                    & Joint distribution of random variables $x$ and $y$ \\
\hline
$f^{\mathcal{L}}$           & Classifier that maps input $x$ to output $y$ \\
\hline
\end{tabular}
\caption{Variable names and definitions.}
\label{tab:var_defs}
\end{table}

\section{Kernel}

A kernel based classifier is used in xPAL which determines the similarity of two data points. The kernel function $\textbf{\textit{K}}(x,x')$ is a function that maps two data points to a real number. The kernel frequency estimate $\textbf{\textit{k}}^{\mathcal{L}}_x$ of an instance $\textbf{\textit{x}}$ is calculated using the labeled instances $\mathcal{L}$. The y-th element of that C-dimensional vector describes the similarity-weighted number of labels of class y.

\begin{equation}
\textbf{\textit{k}}^{\mathcal{L}}_x,y = \sum_{(x',y') \in \mathcal{L}} \mathbb{1}_{y=y'} \textbf{\textit{K}}(x,x')
\label{eq:kernel}
\end{equation}

The Parzen Window Classifier uses the labeled data for training and predicts the most frequent class. 

\begin{equation}
f^{\mathcal{L}}(x) = \underset{y \in \mathcal{Y}}{\arg \max} \left( \textbf{\textit{k}}^{\mathcal{L}}_{x,y} \right) \hbox{.}
\end{equation}

\section{Risk}

For xPAL, Kottke et al. use the classifications error as the performance measure and minimize the zero-one loss. The risk describes the expected value of the loss relative to the joint distribution given some classifier. The zero-one loss returns 0 if the prediction from the classifier is equal to the true class else it returns 1.

\begin{flalign}
\textit{R}(f^{\mathcal{L}}) &= \underset{p(x,y)}{\mathbb{E}} [ \textbf{\textit{L}}(y,f^{\mathcal{L}}(x)) ] \\
&= \underset{p(x)}{\mathbb{E}} \left[ \underset{p(y|x)}{\mathbb{E}} [ \textbf{\textit{L}}(y,f^{\mathcal{L}}(x)) ] \right] \\
\textbf{\textit{L}}(y,f^{\mathcal{L}}(x)) &= \mathbb{1}_{f^{\mathcal{L}}(x)\neq y} 
\end{flalign}

Because it is not known how the data is generated Kottke et al. use a Monte-Carlo integration with all the data $\mathcal{E}$ to represent the generator. The empirical risk $\textit{R}_{\mathcal{E}}$ is the average of the loss over all the data points in the dataset.

\begin{flalign}
\textit{R}_{\mathcal{E}}(f^{\mathcal{L}}) &= \frac{1}{|\mathcal{E}|} \sum_{x \in \mathcal{E}} \underset{p(y|x)}{\mathbb{E}} \left[ \textbf{\textit{L}}(y,f^{\mathcal{L}}(x)) \right] \\
&= \frac{1}{|\mathcal{E}|} \sum_{x \in \mathcal{E}} \sum_{y \in \mathcal{Y}} p(y|x) \textbf{\textit{L}}(y,f^{\mathcal{L}}(x))
\end{flalign}


\section{Conjugate Prior}

The conditional class probability $p(y|x)$ depends on the ground truth which is unkown. As a result the conditional class probability is exactly the y-th element of the unknown ground truth vector $\textit{\textbf{p}}$. The nearby labels from $\mathcal{L}$ can be used to estimate the ground truth $\textit{\textbf{p}}$ becuase the oracle provides the labels according to $\textit{p}$. If we assume a smooth distribution then the estimate is relatively close to the ground truth if we have enough labeled instances. 

\begin{equation}
p(y|x) = p(y|t(x)) = p(y|\textit{\textbf{p}}) = \text{Cat} (y|\textbf{p}) = p_y
\end{equation}

A Bayesian approach is used for estimation by calculating the posterior predictive distribution (calculating the expected value over all possible ground truth values). The probability of y given some x is approximately equal to the kernel frequency estimate of x. 

\begin{equation}
p(y|x) \approx  p(y | \textbf{\textit{k}}^{\mathcal{L}}_x) = \underset{p(p|\textbf{\textit{k}}^{\mathcal{L}}_x)}{\mathbb{E}} \left[ p_y \right] = \int p(p|\textbf{\textit{k}}^{\mathcal{L}}_x) p_y dp
\label{eq:kernel_estimate}
\end{equation}

Bayes theorem is then used to determine the posterior probability of the ground truth at instance x in Equation \ref{eq:bayes}. The likelihood $p(\textbf{\textit{k}}^{\mathcal{L}}_x|p)$ is a multinomial distribution because each label has been drawn from $Cat(y|p)$. A prior is introduced and selected as a Dirichlet distribution with $\alpha \in \mathbb{R}^C$ as this is the conjugate prior of the multinomial distribution. An indifferent prior is choosen and each element of alpha is set to the same value. The Dirichlet distribution is an analytical solution for the posterior when the conjugate prior of the multinomial likelihood are used. 

\begin{flalign}
\label{eq:bayes}
p(p|\textbf{\textit{k}}^{\mathcal{L}}_x) &= \frac{p(\textbf{\textit{k}}^{\mathcal{L}}_x|p)p(p)}{p(\textbf{\textit{k}}^{\mathcal{L}}_x)} \\
&= \frac{\text{Mult}(\textbf{\textit{k}}^{\mathcal{L}}_x|p) \cdot \text{Dir}(p|\alpha)}{\int \text{Mult}(\textbf{\textit{k}}^{\mathcal{L}}_x|p) \cdot \text{Dir}(p|\alpha) dp} \\
&= \text{Dir}(p|\textbf{\textit{k}}^{\mathcal{L}}_x + \alpha)
\end{flalign}

The conditional class probability is determined next from Equation \ref{eq:kernel_estimate}. It is calculated with the expected value of the Dirichlet distribution.

\begin{flalign}
p(y|\textbf{\textit{k}}^{\mathcal{L}}_x) &= \underset{\text{Dir}(p|\textbf{\textit{k}}^{\mathcal{L}}_x+ \alpha)}{\mathbb{E}} \left[ p_y \right] \\
&= \int \text{Dir}(p|\textbf{\textit{k}}^{\mathcal{L}}_x + \alpha) p_y dp = \frac{\textbf{(\textit{k}}^{\mathcal{L}}_x + \alpha)_y}{||\textbf{\textit{k}}^{\mathcal{L}}_x + \alpha ||_1}
\end{flalign}

The last term is the y-th element of the normalized vector. The 1-norm is used to normalize the vector.

\section{Risk Difference Using the Conjugate Prior}

\begin{equation}
    \hat{R}_{\mathcal{E}} (f^{\mathcal{L}^+}, f^{\mathcal{L}}, \mathcal{L}^+) = \hat{R}_{\mathcal{E}} (f^{\mathcal{L}^+}, \mathcal{L}^+) - \hat{R}_{\mathcal{E}} (f^{\mathcal{L}}, \mathcal{L}^+)
\end{equation}

\begin{equation}
    = \frac{1}{|\mathcal{E}|} \sum_{x \in \mathcal{E}} \sum_{y \in \mathcal{Y}}  \frac{\textbf{(\textit{k}}^{\mathcal{L}}_x + \alpha)_y}{||\textbf{\textit{k}}^{\mathcal{L}}_x + \alpha ||_1} \cdot \left( \textbf{\textit{L}} (y,f^{\mathcal{L}^+}(x)) - \textbf{\textit{L}}(y,f^{\mathcal{L}}(x)) \right)
\end{equation}

\section{Expected Probabilistic Gain}

\begin{equation}
    \text{xgain}(x_c, \mathcal{L}, \mathcal{E}) = \underset{p(y_c|\textbf{\textit{k}}^{\mathcal{L}}_{x_c})}{\mathbb{E}} \left[ - \Delta \hat{R}_{\mathcal{E}} (f^{\mathcal{L}^+}, f^{\mathcal{L}}, \mathcal{L}^+ \right]
\end{equation}

\begin{equation}
    = - \sum_{y \in \mathcal{Y}}  \frac{\textbf{(\textit{k}}^{\mathcal{L}}_x + \beta)_y}{||\textbf{\textit{k}}^{\mathcal{L}}_x + \beta ||_1} \cdot \frac{1}{|\mathcal{E}|} \sum_{x \in \mathcal{E}} \sum_{y \in \mathcal{Y}}  \frac{\textbf{(\textit{k}}^{\mathcal{L}^+}_x + \alpha)_y}{||\textbf{\textit{k}}^{\mathcal{L}^+}_x + \alpha ||_1} \cdot \left( \textbf{\textit{L}} (y,f^{\mathcal{L}^+}(x)) - \textbf{\textit{L}}(y,f^{\mathcal{L}}(x)) \right)
\end{equation}

\section{xPAL Selection Strategy}

\begin{equation}
    x^*_c = \underset{x_c \in \mathcal{U}}{\arg \max} \left( \text{xgain} (x_c, \mathcal{L}, \mathcal{E}) \right)
\end{equation}
