{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "047039f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import thesis_utils as tu\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import seaborn as sns\n",
    "from thesis_utils import data_prep\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics.pairwise import pairwise_kernels\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "data = tu.Dataset()\n",
    "td_df = data.translated_data\n",
    "sd_df = data.site_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e3016aa8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Food And Drink    217\n",
      "Car                74\n",
      "Groceries          65\n",
      "Health             58\n",
      "Travel             53\n",
      "Name: category, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "m_df = pd.merge(td_df[['site_data_id', 'original_language']], sd_df[['id', 'category','origin']], left_on='site_data_id', right_on='id')\n",
    "original = m_df[m_df['origin']=='original']\n",
    "additional = m_df[m_df['origin']=='additional']\n",
    "\n",
    "fdf = m_df.groupby('category').filter(lambda x: x['category'].value_counts() > 50)\n",
    "print(fdf['category'].value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6364e40",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y, vect = data_prep(data)\n",
    "le = LabelEncoder()\n",
    "y = le.fit_transform(y)\n",
    "train_X, test_X, train_y, test_y = train_test_split(X, y, test_size=0.25, random_state=42, stratify=y)\n",
    "\n",
    "models = [\n",
    "    LinearSVC(),\n",
    "    MLPClassifier(random_state=1, max_iter=500, hidden_layer_sizes=1000),\n",
    "    SVC(kernel='precomputed'),\n",
    "    GradientBoostingClassifier(n_estimators=100, learning_rate=1.0, max_depth=1, random_state=0),\n",
    "    LogisticRegression(random_state=0),\n",
    "    MultinomialNB(),\n",
    "    RandomForestClassifier(n_estimators=200, max_depth=3, random_state=0),\n",
    "]\n",
    "\n",
    "CV = 5\n",
    "cv_df = pd.DataFrame(index=range(CV * len(models)))\n",
    "entries = []\n",
    "for model in models:\n",
    "    model_name = model.__class__.__name__\n",
    "    if model_name == 'SVC':\n",
    "        train_X_cosine = pairwise_kernels(train_X, metric='cosine')\n",
    "        accuracies = cross_val_score(model, train_X_cosine, train_y, scoring='accuracy', cv=CV)\n",
    "    else:\n",
    "        accuracies = cross_val_score(model, train_X, train_y, scoring='accuracy', cv=CV)\n",
    "    for fold_idx, accuracy in enumerate(accuracies):\n",
    "        entries.append((model_name, fold_idx, accuracy))\n",
    "\n",
    "cv_df = pd.DataFrame(entries, columns=['Classifier', 'fold_idx', 'Accuracy'])\n",
    "sns.boxplot(x='Classifier', y='Accuracy', data=cv_df)\n",
    "# sns.stripplot(x='Classifier', y='Accuracy', data=cv_df, \n",
    "#         size=5, jitter=True, edgecolor=\"gray\", linewidth=1)\n",
    "# change sns x axis labels to be more readable\n",
    "plt.xticks(rotation=90)\n",
    "plt.tight_layout()\n",
    "plt.savefig('plot_explore_classifiers.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3448657f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(cv_df.groupby('model_name').accuracy.mean())\n",
    "# data_df = pd.merge(td_df[['site_data_id', 'original_language']], sd_df[['id', 'category', 'origin']], left_on='site_data_id', right_on='id')\n",
    "# filtered_df.groupby('category').count()\n",
    "# original['category'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4775c3c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "original = m_df[(m_df['origin'] == 'original') & (m_df['original_language'] == 'en')]\n",
    "categories = m_df['category'].unique()\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "bar_width = 1\n",
    "opacity = 0.5\n",
    "\n",
    "df1_c = original['category'].value_counts()\n",
    "# Add missing categories and fill with zeros\n",
    "missing_categories = set(categories) - set(df1_c.index)\n",
    "for category in missing_categories:\n",
    "    df1_c = df1_c.append(pd.Series([0], index=[category]))\n",
    "# Sort by category\n",
    "df1_c = df1_c.sort_index()\n",
    "category_labels = sorted(categories, key=lambda s: s.split()[0])\n",
    "ax.bar(df1_c.index, df1_c.values, bar_width, alpha=opacity, color='b', label='Original Data')\n",
    "ax.set_xlabel('Categories')\n",
    "ax.set_ylabel('Count')\n",
    "ax.set_xticks(np.arange(len(categories)))\n",
    "ax.set_xticklabels(category_labels, rotation=90, fontsize=8)\n",
    "ax.legend()\n",
    "fig.tight_layout()\n",
    "plt.show()\n",
    "plt.close(fig)\n",
    "\n",
    "# df1_c shows counts for the plot sum is 275"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42d76c5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(df1_c['category'].value_counts())\n",
    "# print(df1_c['category'].value_counts().sum())\n",
    "df1_c\n",
    "df1_c.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f05ca55a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clustermap of original data (interesting but maybe not super useful)\n",
    "\n",
    "X, y, v = data_prep(data)\n",
    "x = X.toarray()\n",
    "df_tfidf = pd.DataFrame(x, columns=v.get_feature_names_out())\n",
    "cm = sns.clustermap(x, cmap=\"vlag\", center=0)\n",
    "plt.xticks(rotation=90)\n",
    "plt.xticks(range(len(df_tfidf.columns)), df_tfidf.columns, fontsize=4)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8779e757",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "23/23 [==============================] - 7s 177ms/step - loss: 2.9884 - accuracy: 0.2427\n",
      "Epoch 2/10\n",
      "23/23 [==============================] - 4s 155ms/step - loss: 2.6664 - accuracy: 0.2734\n",
      "Epoch 3/10\n",
      "23/23 [==============================] - 3s 151ms/step - loss: 2.6367 - accuracy: 0.2734\n",
      "Epoch 4/10\n",
      "23/23 [==============================] - 3s 151ms/step - loss: 2.6330 - accuracy: 0.2734\n",
      "Epoch 5/10\n",
      "23/23 [==============================] - 3s 146ms/step - loss: 2.6339 - accuracy: 0.2734\n",
      "Epoch 6/10\n",
      "23/23 [==============================] - 3s 149ms/step - loss: 2.6336 - accuracy: 0.2734\n",
      "Epoch 7/10\n",
      "23/23 [==============================] - 3s 146ms/step - loss: 2.6334 - accuracy: 0.2734\n",
      "Epoch 8/10\n",
      "23/23 [==============================] - 3s 145ms/step - loss: 2.6305 - accuracy: 0.2734\n",
      "Epoch 9/10\n",
      "23/23 [==============================] - 3s 145ms/step - loss: 2.6304 - accuracy: 0.2734\n",
      "Epoch 10/10\n",
      "23/23 [==============================] - 3s 148ms/step - loss: 2.6311 - accuracy: 0.2734\n",
      "8/8 [==============================] - 1s 40ms/step\n",
      "8/8 [==============================] - 1s 59ms/step - loss: 2.6204 - accuracy: 0.2750\n",
      "[2.620396375656128, 0.2750000059604645]\n"
     ]
    }
   ],
   "source": [
    "# coding=utf8\n",
    "#!/usr/bin/env python3\n",
    "import os\n",
    "os.environ.setdefault(\"TF_CPP_MIN_LOG_LEVEL\", \"2\")  # Report only TF errors by default\n",
    "import warnings\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "warnings.simplefilter(action='ignore', category=UserWarning)\n",
    "\n",
    "data = tu.Dataset()\n",
    "texts, labels = tu.data_prep_fixed(data, origin_filter=None)\n",
    "train_X, test_X, train_y, test_y = train_test_split(texts, labels, test_size=0.25, random_state=42, stratify=labels)\n",
    "\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(train_X)\n",
    "word_index = tokenizer.word_index\n",
    "\n",
    "sequences = tokenizer.texts_to_sequences(train_X)\n",
    "max_length = max(len(sequence) for sequence in sequences)\n",
    "padded_sequences = pad_sequences(sequences, maxlen=max_length, padding='post')\n",
    "\n",
    "label_encoder = {}\n",
    "for i, label in enumerate(set(train_y)):\n",
    "    label_encoder[label] = i\n",
    "    \n",
    "encoded_labels = np.array([label_encoder[label] for label in train_y])\n",
    "num_classes = len(set(encoded_labels))\n",
    "one_hot_labels = tf.keras.utils.to_categorical(encoded_labels, num_classes)\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(len(word_index)+1, 32, input_length=max_length),\n",
    "    tf.keras.layers.LSTM(64, dropout=0.2),\n",
    "    tf.keras.layers.Dense(num_classes, activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(\n",
    "            loss=tf.keras.losses.categorical_crossentropy, \n",
    "            optimizer=tf.keras.optimizers.Adam(jit_compile=False), \n",
    "            metrics=['accuracy'])\n",
    "model.fit(padded_sequences, one_hot_labels, epochs=10, verbose=1)\n",
    "\n",
    "new_sequences = tokenizer.texts_to_sequences(test_X)\n",
    "new_padded_sequences = pad_sequences(new_sequences, maxlen=max_length, padding='post')\n",
    "predictions = model.predict(new_padded_sequences)\n",
    "accuracy = model.evaluate(new_padded_sequences, tf.keras.utils.to_categorical(np.array([label_encoder[label] for label in test_y]), num_classes))\n",
    "print(accuracy)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7a3711d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [2.6221745014190674, 0.2750000059604645] with fancy stop words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b23c8705",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "The input `table` must be of shape (2, 2).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/Users/mitchellborchers/Documents/git/charles-university-thesis/code/scrapy_tut/tutorial/Testing Notebook.ipynb Cell 10\u001b[0m in \u001b[0;36m<cell line: 49>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/mitchellborchers/Documents/git/charles-university-thesis/code/scrapy_tut/tutorial/Testing%20Notebook.ipynb#X12sZmlsZQ%3D%3D?line=47'>48</a>\u001b[0m p_values \u001b[39m=\u001b[39m {}\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/mitchellborchers/Documents/git/charles-university-thesis/code/scrapy_tut/tutorial/Testing%20Notebook.ipynb#X12sZmlsZQ%3D%3D?line=48'>49</a>\u001b[0m \u001b[39mfor\u001b[39;00m keyword, contingency_table \u001b[39min\u001b[39;00m contingency_tables\u001b[39m.\u001b[39mitems():\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/mitchellborchers/Documents/git/charles-university-thesis/code/scrapy_tut/tutorial/Testing%20Notebook.ipynb#X12sZmlsZQ%3D%3D?line=49'>50</a>\u001b[0m     odds_ratio, p_value \u001b[39m=\u001b[39m fisher_exact(contingency_table)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/mitchellborchers/Documents/git/charles-university-thesis/code/scrapy_tut/tutorial/Testing%20Notebook.ipynb#X12sZmlsZQ%3D%3D?line=50'>51</a>\u001b[0m     p_values[keyword] \u001b[39m=\u001b[39m p_value\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/mitchellborchers/Documents/git/charles-university-thesis/code/scrapy_tut/tutorial/Testing%20Notebook.ipynb#X12sZmlsZQ%3D%3D?line=52'>53</a>\u001b[0m sorted_keywords \u001b[39m=\u001b[39m \u001b[39msorted\u001b[39m(p_values\u001b[39m.\u001b[39mitems(), key\u001b[39m=\u001b[39m\u001b[39mlambda\u001b[39;00m x: x[\u001b[39m1\u001b[39m])\n",
      "File \u001b[0;32m~/.virtualenvs/thesis/lib/python3.9/site-packages/scipy/stats/_stats_py.py:4278\u001b[0m, in \u001b[0;36mfisher_exact\u001b[0;34m(table, alternative)\u001b[0m\n\u001b[1;32m   4276\u001b[0m c \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39masarray(table, dtype\u001b[39m=\u001b[39mnp\u001b[39m.\u001b[39mint64)\n\u001b[1;32m   4277\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m c\u001b[39m.\u001b[39mshape \u001b[39m==\u001b[39m (\u001b[39m2\u001b[39m, \u001b[39m2\u001b[39m):\n\u001b[0;32m-> 4278\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mThe input `table` must be of shape (2, 2).\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m   4280\u001b[0m \u001b[39mif\u001b[39;00m np\u001b[39m.\u001b[39many(c \u001b[39m<\u001b[39m \u001b[39m0\u001b[39m):\n\u001b[1;32m   4281\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mAll values in `table` must be nonnegative.\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[0;31mValueError\u001b[0m: The input `table` must be of shape (2, 2)."
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from string import punctuation\n",
    "from nltk.probability import FreqDist\n",
    "from scipy.stats import fisher_exact\n",
    "\n",
    "def preprocess_text(text):\n",
    "    # tokenize the text\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    \n",
    "    # remove stop words and punctuation\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [word.lower() for word in tokens if word.lower() not in stop_words and word not in punctuation]\n",
    "    return tokens\n",
    "\n",
    "\n",
    "def create_freq_dist(tokens):\n",
    "    freq_dist = FreqDist(tokens)\n",
    "    return freq_dist\n",
    "\n",
    "X, y = tu.data_prep_fixed(data, origin_filter=None)\n",
    "# create a dataframe of the text data and the labels\n",
    "fisher_df = pd.DataFrame({'text': X, 'label': y})\n",
    "grouped = fisher_df.groupby('label')\n",
    "\n",
    "freq_dists = {}\n",
    "for label, group in grouped:\n",
    "    freq_dists[label] = {}\n",
    "    for text in group['text']:\n",
    "        tokens = preprocess_text(text)\n",
    "        freq_dist = create_freq_dist(tokens)\n",
    "        for token, freq in freq_dist.items():\n",
    "            if token not in freq_dists[label]:\n",
    "                freq_dists[label][token] = 0\n",
    "            freq_dists[label][token] += freq\n",
    "\n",
    "categories = list(freq_dists.keys())\n",
    "\n",
    "contingency_tables = {}\n",
    "for keyword in freq_dists[categories[0]]:\n",
    "    contingency_table = []\n",
    "    for category in categories:\n",
    "        freq = freq_dists[category][keyword] if keyword in freq_dists[category] else 0\n",
    "        nonfreq = sum(freq_dists[category].values()) - freq\n",
    "        contingency_table.append([freq, nonfreq])\n",
    "    contingency_tables[keyword] = contingency_table\n",
    "\n",
    "p_values = {}\n",
    "for keyword, contingency_table in contingency_tables.items():\n",
    "    odds_ratio, p_value = fisher_exact(contingency_table)\n",
    "    p_values[keyword] = p_value\n",
    "\n",
    "sorted_keywords = sorted(p_values.items(), key=lambda x: x[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "446667db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Veski tavern for bread and fun Veski tavern for bread and fun Move to the contents Home Bread Lust Album Veski Kodu Veski Tavern is like a summer milkshake in the middle of Tondi settlement It's fun to have breakfast behind long farm tables\n",
      "[array(['tables', 'farm', 'long', 'breakfast', 'settlement', 'tondi',\n",
      "       'middle', 'milkshake', 'summer', 'like', 'kodu', 'album', 'lust',\n",
      "       'home', 'contents', 'fun', 'bread', 'tavern', 'veski'],\n",
      "      dtype='<U81')]\n",
      "tables farm long breakfast settlement tondi middle milkshake summer like kodu album lust home contents fun bread tavern veski\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import thesis_utils as tu\n",
    "# Load some text data\n",
    "data = tu.Dataset()\n",
    "X, y = tu.data_prep_fixed(data, origin_filter=None)\n",
    "print(X[0])\n",
    "# Create TfidfVectorizer with stop words removed\n",
    "v = TfidfVectorizer(stop_words='english')\n",
    "\n",
    "# Fit and transform the text data using the vectorizer\n",
    "X_train = v.fit_transform(X)\n",
    "# transfomr x back into text \n",
    "print(v.inverse_transform(X_train[0]))\n",
    "# join the text back together\n",
    "print(' '.join(v.inverse_transform(X_train[0])[0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dcd57bbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Patria Finance Search Login CS SK EN Why Patria Fi\n",
      "1 Home TESLA investiční společnost\n",
      "2 Sirius investment company Please wait Investment a\n",
      "3 Pravda Capital Investment for those who strive to \n",
      "4 Summit Financial Group Investments Financial Plann\n",
      "5 Advisor Financial Services of America FSA Warren t\n",
      "6 Home Financial Solutions of Michigan fsmfinancial \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "data = tu.Dataset()\n",
    "X, y = tu.data_prep_fixed(data, origin_filter=None)\n",
    "# join X and y into a dataframe\n",
    "df = pd.DataFrame({'text': X, 'label': y})\n",
    "# show all text with label consumer goods\n",
    "res = df[df['label'] == 'Investments']['text'].values\n",
    "\n",
    "for i, r in enumerate(res):\n",
    "    print(i, r[:50])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
